{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T16:26:46.241312Z",
     "start_time": "2023-08-09T16:26:46.198469Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Discrete, Box, Dict\n",
    "from gymnasium.utils import seeding\n",
    "\n",
    "\n",
    "def calculate_manhattan_distance(current_pos, goal_pos):\n",
    "    return abs(current_pos[0] - goal_pos[0]) + abs(current_pos[1] - goal_pos[1])\n",
    "\n",
    "\n",
    "def find_tile_position(grid, tile):\n",
    "    row, col = np.where(grid == tile)\n",
    "    return row[0], col[0]\n",
    "\n",
    "\n",
    "def total_manhattan_distance(current_state):\n",
    "    goal_state = np.arange(1, 17).reshape(4, 4)\n",
    "    total_manhattan_distance = 0\n",
    "    for num in range(1, 16):\n",
    "        current_pos = find_tile_position(current_state, num)\n",
    "        goal_pos = find_tile_position(goal_state, num)\n",
    "        total_manhattan_distance += calculate_manhattan_distance(current_pos, goal_pos)\n",
    "    return total_manhattan_distance\n",
    "\n",
    "\n",
    "def is_solvable(state):\n",
    "    permutation = state.flatten()\n",
    "    inversions = 0\n",
    "    for i in range(len(permutation)):\n",
    "        for j in range(i + 1, len(permutation)):\n",
    "            if permutation[i] == 0 or permutation[j] == 0:\n",
    "                continue\n",
    "            if permutation[i] > permutation[j]:\n",
    "                inversions += 1\n",
    "    num_rows = state.shape[0]\n",
    "    if num_rows % 2 == 0:\n",
    "        empty_row = np.where(state == 0)[0][0]\n",
    "        return (inversions + empty_row) % 2 == 0\n",
    "    else:\n",
    "        return inversions % 2 == 0\n",
    "\n",
    "\n",
    "class FifteenPuzzleEnv(gym.Env):\n",
    "    def __init__(self, config=None):\n",
    "        self.current_steps = 0\n",
    "        self.max_episode_steps = 100\n",
    "        self.action_space = Discrete(4)  # 0: left, 1: up, 2: right, 3: down\n",
    "        self.grid_size = 4\n",
    "        self.observation_space = Box(low=0, high=self.grid_size ** 2 - 1, shape=(self.grid_size, self.grid_size),\n",
    "                                     dtype=np.int32)\n",
    "        self.seed()\n",
    "        self.reset()\n",
    "        self.spec = gym.envs.registration.EnvSpec(\"fp\", max_episode_steps=100, reward_threshold=100)\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.state = np.arange(self.grid_size ** 2)\n",
    "        self.np_random.shuffle(self.state)\n",
    "        while not is_solvable(self.state):\n",
    "            self.np_random.shuffle(self.state)\n",
    "        self.current_steps = 0\n",
    "        self.state = self.state.reshape((self.grid_size, self.grid_size))\n",
    "        self.zero_pos = np.argwhere(self.state == 0)[0]\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.current_steps += 1\n",
    "        distance_1 = total_manhattan_distance(self.state)\n",
    "        valid_move = self.move(action)\n",
    "        d = self.is_solved() or self.current_steps >= self.max_episode_steps\n",
    "        distance_2 = total_manhattan_distance(self.state)\n",
    "        r = distance_1 - distance_2\n",
    "        if not valid_move:\n",
    "            r -= 5\n",
    "        return self.state, r, d, False, {}\n",
    "\n",
    "    def move(self, action):\n",
    "        new_zero_pos = np.array(self.zero_pos)\n",
    "        if action == 0:  # left\n",
    "            new_zero_pos[1] -= 1\n",
    "        elif action == 1:  # up\n",
    "            new_zero_pos[0] -= 1\n",
    "        elif action == 2:  # right\n",
    "            new_zero_pos[1] += 1\n",
    "        elif action == 3:  # down\n",
    "            new_zero_pos[0] += 1\n",
    "\n",
    "        if (0 <= new_zero_pos[0] < self.grid_size) and (0 <= new_zero_pos[1] < self.grid_size):\n",
    "            self.state[self.zero_pos[0], self.zero_pos[1]], self.state[new_zero_pos[0], new_zero_pos[1]] = (\n",
    "                self.state[new_zero_pos[0], new_zero_pos[1]],\n",
    "                self.state[self.zero_pos[0], self.zero_pos[1]],\n",
    "            )\n",
    "            self.zero_pos = new_zero_pos\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def is_solved(self):\n",
    "        goal_state = np.arange(1, 17).reshape((self.grid_size, self.grid_size))\n",
    "        goal_state[3][3] = 0\n",
    "        return np.array_equal(self.state, goal_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T16:26:48.093203Z",
     "start_time": "2023-08-09T16:26:48.090369Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tianshou'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtensorboard\u001b[39;00m \u001b[39mimport\u001b[39;00m SummaryWriter\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtianshou\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mts\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mray\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtune\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistry\u001b[39;00m \u001b[39mimport\u001b[39;00m register_env\n\u001b[1;32m      7\u001b[0m register_env(\u001b[39m\"\u001b[39m\u001b[39mfp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mlambda\u001b[39;00m config: FifteenPuzzleEnv())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tianshou'"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "register_env(\"fp\", lambda config: FifteenPuzzleEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T16:26:48.248739Z",
     "start_time": "2023-08-09T16:26:48.246547Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr, epoch, batch_size = 1e-3, 10, 64\n",
    "train_num, test_num = 10, 100\n",
    "gamma, n_step, target_freq = 0.9, 3, 320\n",
    "buffer_size = 20000\n",
    "eps_train, eps_test = 0.1, 0.05\n",
    "step_per_epoch, step_per_collect = 10000, 10\n",
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))  # TensorBoard is supported!\n",
    "# For other loggers: https://tianshou.readthedocs.io/en/master/tutorials/logger.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T16:26:49.819765Z",
     "start_time": "2023-08-09T16:26:49.785632Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EnvSpec.__init__() missing 1 required positional argument: 'entry_point'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# you can also try with SubprocVectorEnv\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_envs \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mDummyVectorEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: FifteenPuzzleEnv() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_num)])\n\u001b[1;32m      3\u001b[0m test_envs \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mDummyVectorEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: FifteenPuzzleEnv() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(test_num)])\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/tianshou/env/venvs.py:443\u001b[0m, in \u001b[0;36mDummyVectorEnv.__init__\u001b[0;34m(self, env_fns, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fns: List[Callable[[], ENV_TYPE]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(env_fns, DummyEnvWorker, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/tianshou/env/venvs.py:153\u001b[0m, in \u001b[0;36mBaseVectorEnv.__init__\u001b[0;34m(self, env_fns, worker_fn, wait_num, timeout)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_fns \u001b[38;5;241m=\u001b[39m env_fns\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# A VectorEnv contains a pool of EnvWorkers, which corresponds to\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# interact with the given envs (one worker <-> one env).\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m [worker_fn(_patch_env_generator(fn)) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_class, EnvWorker)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/tianshou/env/venvs.py:153\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env_fns \u001b[38;5;241m=\u001b[39m env_fns\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# A VectorEnv contains a pool of EnvWorkers, which corresponds to\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# interact with the given envs (one worker <-> one env).\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m [worker_fn(_patch_env_generator(fn)) \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m env_fns]\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_class, EnvWorker)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/tianshou/env/worker/dummy.py:13\u001b[0m, in \u001b[0;36mDummyEnvWorker.__init__\u001b[0;34m(self, env_fn)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_fn: Callable[[], gym\u001b[38;5;241m.\u001b[39mEnv]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m env_fn()\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(env_fn)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/tianshou/env/venvs.py:48\u001b[0m, in \u001b[0;36m_patch_env_generator.<locals>.patched\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpatched\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m gym\u001b[38;5;241m.\u001b[39mEnv:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\n\u001b[1;32m     45\u001b[0m         fn\n\u001b[1;32m     46\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnv generators that are provided to vector environemnts must be callable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m     env \u001b[38;5;241m=\u001b[39m fn()\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env, (gym\u001b[38;5;241m.\u001b[39mEnv, PettingZooEnv)):\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m env\n",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# you can also try with SubprocVectorEnv\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_envs \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mDummyVectorEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: FifteenPuzzleEnv() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(train_num)])\n\u001b[1;32m      3\u001b[0m test_envs \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mDummyVectorEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: FifteenPuzzleEnv() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(test_num)])\n",
      "Cell \u001b[0;32mIn[15], line 54\u001b[0m, in \u001b[0;36mFifteenPuzzleEnv.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39menvs\u001b[38;5;241m.\u001b[39mregistration\u001b[38;5;241m.\u001b[39mEnvSpec(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfp\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_episode_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, reward_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: EnvSpec.__init__() missing 1 required positional argument: 'entry_point'"
     ]
    }
   ],
   "source": [
    "# you can also try with SubprocVectorEnv\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: FifteenPuzzleEnv() for _ in range(train_num)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: FifteenPuzzleEnv() for _ in range(test_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T16:21:25.187408Z",
     "start_time": "2023-08-09T16:21:25.162210Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tianshou.utils.net.common import Net\n",
    "# you can define other net by following the API:\n",
    "# https://tianshou.readthedocs.io/en/master/tutorials/dqn.html#build-the-network\n",
    "env = FifteenPuzzleEnv()\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "net = Net(state_shape=state_shape, action_shape=action_shape, hidden_sizes=[128, 128, 128])\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T16:21:28.173240Z",
     "start_time": "2023-08-09T16:21:26.820657Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy = ts.policy.DQNPolicy(net, optim, gamma, n_step, target_update_freq=target_freq)\n",
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(buffer_size, train_num), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)  # because DQN uses epsilon-greedy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T16:23:31.537014Z",
     "start_time": "2023-08-09T16:23:31.531042Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(env.spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T16:23:33.065919Z",
     "start_time": "2023-08-09T16:23:31.620401Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1:  10%|9         | 990/10000 [00:00<00:04, 1879.65it/s, env_step=990, len=0, loss=20.486, n/ep=0, n/st=10, rew=0.00]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reward_threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moffpolicy_trainer(\n\u001b[1;32m      2\u001b[0m     policy, train_collector, test_collector, epoch, step_per_epoch, step_per_collect,\n\u001b[1;32m      3\u001b[0m     test_num, batch_size, update_per_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m step_per_collect,\n\u001b[1;32m      4\u001b[0m     train_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, env_step: policy\u001b[38;5;241m.\u001b[39mset_eps(eps_train),\n\u001b[1;32m      5\u001b[0m     test_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, env_step: policy\u001b[38;5;241m.\u001b[39mset_eps(eps_test),\n\u001b[1;32m      6\u001b[0m     stop_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m mean_rewards: mean_rewards \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mspec\u001b[38;5;241m.\u001b[39mreward_threshold,\n\u001b[1;32m      7\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training! Use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/tianshou/trainer/offpolicy.py:133\u001b[0m, in \u001b[0;36moffpolicy_trainer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moffpolicy_trainer\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for OffPolicyTrainer run method.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03m    It is identical to ``OffpolicyTrainer(...).run()``.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;03m    :return: See :func:`~tianshou.trainer.gather_info`.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OffpolicyTrainer(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/tianshou/trainer/base.py:441\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 441\u001b[0m     deque(\u001b[38;5;28mself\u001b[39m, maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     info \u001b[38;5;241m=\u001b[39m gather_info(\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_collector, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector,\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_reward_std\n\u001b[1;32m    445\u001b[0m     )\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/tianshou/trainer/base.py:288\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m result: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_collector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     data, result, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_fn_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step()\n\u001b[1;32m    289\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn/st\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_fn_flag:\n",
      "File \u001b[0;32m~/anaconda3/envs/rl/lib/python3.11/site-packages/tianshou/trainer/base.py:402\u001b[0m, in \u001b[0;36mBaseTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    394\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv_step\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_step),\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrew\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_rew\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn/st\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mint\u001b[39m(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn/st\u001b[39m\u001b[38;5;124m\"\u001b[39m])),\n\u001b[1;32m    400\u001b[0m }\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn/ep\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_in_train \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_fn(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrew\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         test_result \u001b[38;5;241m=\u001b[39m test_episode(\n\u001b[1;32m    405\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_fn, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch,\n\u001b[1;32m    406\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_per_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_step\n\u001b[1;32m    407\u001b[0m         )\n",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(mean_rewards)\u001b[0m\n\u001b[1;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39moffpolicy_trainer(\n\u001b[1;32m      2\u001b[0m     policy, train_collector, test_collector, epoch, step_per_epoch, step_per_collect,\n\u001b[1;32m      3\u001b[0m     test_num, batch_size, update_per_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m step_per_collect,\n\u001b[1;32m      4\u001b[0m     train_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, env_step: policy\u001b[38;5;241m.\u001b[39mset_eps(eps_train),\n\u001b[1;32m      5\u001b[0m     test_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, env_step: policy\u001b[38;5;241m.\u001b[39mset_eps(eps_test),\n\u001b[0;32m----> 6\u001b[0m     stop_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m mean_rewards: mean_rewards \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mspec\u001b[38;5;241m.\u001b[39mreward_threshold,\n\u001b[1;32m      7\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training! Use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reward_threshold'"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.offpolicy_trainer(\n",
    "    policy, train_collector, test_collector, epoch, step_per_epoch, step_per_collect,\n",
    "    test_num, batch_size, update_per_step=1 / step_per_collect,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(eps_train),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(eps_test),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold,\n",
    "    logger=logger)\n",
    "print(f'Finished training! Use {result[\"duration\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T16:16:43.726538Z",
     "start_time": "2023-08-09T16:16:43.713759Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(policy.state_dict(), 'dqn.pth')\n",
    "policy.load_state_dict(torch.load('dqn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-09T16:16:49.374724Z",
     "start_time": "2023-08-09T16:16:44.067435Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spacefarers/anaconda3/envs/rl/lib/python3.11/site-packages/gymnasium/envs/classic_control/cartpole.py:213: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n/ep': 1,\n",
       " 'n/st': 180,\n",
       " 'rews': array([180.]),\n",
       " 'lens': array([180]),\n",
       " 'idxs': array([0]),\n",
       " 'rew': 180.0,\n",
       " 'len': 180.0,\n",
       " 'rew_std': 0.0,\n",
       " 'len_std': 0.0}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(eps_test)\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "collector.collect(n_episode=1, render=1 / 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
