{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "\n",
    "class FifteenPuzzleEnv(gym.Env):\n",
    "    def __init__(self,env_config):\n",
    "        self.grid_size = 4\n",
    "        self.action_space = spaces.Discrete(4)  # 0: left, 1: up, 2: right, 3: down\n",
    "        self.observation_space = spaces.Box(low=0, high=self.grid_size - 1, shape=(self.grid_size, self.grid_size), dtype=np.int32)\n",
    "        self.seed()\n",
    "        self.reset()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.arange(self.grid_size ** 2)\n",
    "        self.np_random.shuffle(self.state)\n",
    "        self.state = self.state.reshape((self.grid_size, self.grid_size))\n",
    "        self.zero_pos = np.argwhere(self.state == 0)[0]\n",
    "        assert self.observation_space.contains(self.state), \"Invalid initial state!\"\n",
    "        print(self.state)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        self.move(action)\n",
    "        done = self.is_solved()\n",
    "        reward = 1.0 if done else 0.0\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def move(self, action):\n",
    "        new_zero_pos = np.array(self.zero_pos)\n",
    "        if action == 0:  # left\n",
    "            new_zero_pos[1] -= 1\n",
    "        elif action == 1:  # up\n",
    "            new_zero_pos[0] -= 1\n",
    "        elif action == 2:  # right\n",
    "            new_zero_pos[1] += 1\n",
    "        elif action == 3:  # down\n",
    "            new_zero_pos[0] += 1\n",
    "\n",
    "        if (0 <= new_zero_pos[0] < self.grid_size) and (0 <= new_zero_pos[1] < self.grid_size):\n",
    "            self.state[self.zero_pos[0], self.zero_pos[1]], self.state[new_zero_pos[0], new_zero_pos[1]] = (\n",
    "                self.state[new_zero_pos[0], new_zero_pos[1]],\n",
    "                self.state[self.zero_pos[0], self.zero_pos[1]],\n",
    "            )\n",
    "            self.zero_pos = new_zero_pos\n",
    "\n",
    "    def is_solved(self):\n",
    "        return np.array_equal(self.state, np.arange(self.grid_size ** 2).reshape((self.grid_size, self.grid_size)))\n",
    "\n",
    "def train_agent(config, stop_criteria):\n",
    "    ray.init(ignore_reinit_error=True, log_to_driver=False)\n",
    "    trainer = PPO(config=config, env=FifteenPuzzleEnv)\n",
    "    tune.run(trainer, stop=stop_criteria)\n",
    "    ray.shutdown()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 22:24:58,966\tINFO worker.py:1474 -- Calling ray.init() again after it has already been called.\n",
      "2023-07-19 22:24:58,967\tWARNING deprecation.py:50 -- DeprecationWarning: `algo = Algorithm(env='<class '__main__.FifteenPuzzleEnv'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class '__main__.FifteenPuzzleEnv'>').build()` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 22:25:01,086\tERROR actor_manager.py:507 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=113045, ip=192.168.0.121, actor_id=31285a927f780c7b10b58f9d01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7cb7e2d5a0>)\n",
      "  File \"/home/spacefarers/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 609, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"/home/spacefarers/.local/lib/python3.10/site-packages/ray/rllib/env/utils.py\", line 133, in _gym_env_creator\n",
      "    env = env_descriptor(env_context)\n",
      "  File \"/tmp/ipykernel_88377/1908796418.py\", line 15, in __init__\n",
      "  File \"/tmp/ipykernel_88377/1908796418.py\", line 26, in reset\n",
      "AssertionError: Invalid initial state!\n",
      "2023-07-19 22:25:01,086\tERROR actor_manager.py:507 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=113046, ip=192.168.0.121, actor_id=8bd060bcfc51582b994ed70f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f282eadd5a0>)\n",
      "  File \"/home/spacefarers/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 609, in __init__\n",
      "    self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "  File \"/home/spacefarers/.local/lib/python3.10/site-packages/ray/rllib/env/utils.py\", line 133, in _gym_env_creator\n",
      "    env = env_descriptor(env_context)\n",
      "  File \"/tmp/ipykernel_88377/1908796418.py\", line 15, in __init__\n",
      "  File \"/tmp/ipykernel_88377/1908796418.py\", line 26, in reset\n",
      "AssertionError: Invalid initial state!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Invalid initial state!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:172\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup(\n\u001b[1;32m    173\u001b[0m         validate_env\u001b[39m=\u001b[39;49mvalidate_env,\n\u001b[1;32m    174\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    175\u001b[0m         num_workers\u001b[39m=\u001b[39;49mnum_workers,\n\u001b[1;32m    176\u001b[0m         local_worker\u001b[39m=\u001b[39;49mlocal_worker,\n\u001b[1;32m    177\u001b[0m     )\n\u001b[1;32m    178\u001b[0m \u001b[39m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39m# constructor).\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:242\u001b[0m, in \u001b[0;36mWorkerSet._setup\u001b[0;34m(self, validate_env, config, num_workers, local_worker)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39m# Create a number of @ray.remote workers.\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_workers(\n\u001b[1;32m    243\u001b[0m     num_workers,\n\u001b[1;32m    244\u001b[0m     validate\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mvalidate_workers_after_construction,\n\u001b[1;32m    245\u001b[0m )\n\u001b[1;32m    247\u001b[0m \u001b[39m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[39m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# the first remote worker (which does have an env).\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:635\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m result\u001b[39m.\u001b[39mok:\n\u001b[0;32m--> 635\u001b[0m     \u001b[39mraise\u001b[39;00m result\u001b[39m.\u001b[39mget()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/utils/actor_manager.py:488\u001b[0m, in \u001b[0;36mFaultTolerantActorManager.__fetch_result\u001b[0;34m(self, remote_actor_ids, remote_calls, tags, timeout_seconds, return_obj_refs, mark_healthy)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 488\u001b[0m     result \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39;49mget(r)\n\u001b[1;32m    489\u001b[0m     remote_results\u001b[39m.\u001b[39madd_result(actor_id, ResultOrError(result\u001b[39m=\u001b[39mresult), tag)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:18\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m auto_init_ray()\n\u001b[0;32m---> 18\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(ray, func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/_private/worker.py:2542\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2541\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2542\u001b[0m             \u001b[39mraise\u001b[39;00m value\n\u001b[1;32m   2544\u001b[0m \u001b[39mif\u001b[39;00m is_individual_id:\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=113045, ip=192.168.0.121, actor_id=31285a927f780c7b10b58f9d01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f7cb7e2d5a0>)\n  File \"/home/spacefarers/.local/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 609, in __init__\n    self.env = env_creator(copy.deepcopy(self.env_context))\n  File \"/home/spacefarers/.local/lib/python3.10/site-packages/ray/rllib/env/utils.py\", line 133, in _gym_env_creator\n    env = env_descriptor(env_context)\n  File \"/tmp/ipykernel_88377/1908796418.py\", line 15, in __init__\n  File \"/tmp/ipykernel_88377/1908796418.py\", line 26, in reset\nAssertionError: Invalid initial state!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m config \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mframework\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnum_workers\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmonitor\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     17\u001b[0m     }\n\u001b[1;32m     18\u001b[0m stop_criteria \u001b[39m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mepisode_reward_mean\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1.0\u001b[39m,  \u001b[39m# Stop when the mean reward reaches 1.0 (solved)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtime_total_s\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m600\u001b[39m,  \u001b[39m# Maximum training time (in seconds)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     }\n\u001b[0;32m---> 22\u001b[0m train_agent(config, stop_criteria)\n",
      "Cell \u001b[0;32mIn[53], line 59\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(config, stop_criteria)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_agent\u001b[39m(config, stop_criteria):\n\u001b[1;32m     58\u001b[0m     ray\u001b[39m.\u001b[39minit(ignore_reinit_error\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, log_to_driver\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 59\u001b[0m     trainer \u001b[39m=\u001b[39m PPO(config\u001b[39m=\u001b[39;49mconfig, env\u001b[39m=\u001b[39;49mFifteenPuzzleEnv)\n\u001b[1;32m     60\u001b[0m     tune\u001b[39m.\u001b[39mrun(trainer, stop\u001b[39m=\u001b[39mstop_criteria)\n\u001b[1;32m     61\u001b[0m     ray\u001b[39m.\u001b[39mshutdown()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:475\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[39m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[39m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[39m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[39m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_metrics \u001b[39m=\u001b[39m {\n\u001b[1;32m    462\u001b[0m     \u001b[39m# TODO: Don't dump sampler results into top-level.\u001b[39;00m\n\u001b[1;32m    463\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mevaluation\u001b[39m\u001b[39m\"\u001b[39m: {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m     },\n\u001b[1;32m    473\u001b[0m }\n\u001b[0;32m--> 475\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    476\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    477\u001b[0m     logger_creator\u001b[39m=\u001b[39;49mlogger_creator,\n\u001b[1;32m    478\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    479\u001b[0m )\n\u001b[1;32m    481\u001b[0m \u001b[39m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[39m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[39m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:170\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout, sync_config)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_local_ip \u001b[39m=\u001b[39m ray\u001b[39m.\u001b[39mutil\u001b[39m.\u001b[39mget_node_ip_address()\n\u001b[0;32m--> 170\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup(copy\u001b[39m.\u001b[39;49mdeepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig))\n\u001b[1;32m    171\u001b[0m setup_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m setup_time \u001b[39m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:601\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[39m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[39mif\u001b[39;00m _init \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    596\u001b[0m     \u001b[39m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[39m# - Run the execution plan to create the local iterator to `next()`\u001b[39;00m\n\u001b[1;32m    598\u001b[0m     \u001b[39m#   in each training iteration.\u001b[39;00m\n\u001b[1;32m    599\u001b[0m     \u001b[39m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    600\u001b[0m     \u001b[39m# has been deprecated.\u001b[39;00m\n\u001b[0;32m--> 601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mworkers \u001b[39m=\u001b[39m WorkerSet(\n\u001b[1;32m    602\u001b[0m         env_creator\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv_creator,\n\u001b[1;32m    603\u001b[0m         validate_env\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalidate_env,\n\u001b[1;32m    604\u001b[0m         default_policy_class\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_default_policy_class(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig),\n\u001b[1;32m    605\u001b[0m         config\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig,\n\u001b[1;32m    606\u001b[0m         num_workers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mnum_rollout_workers,\n\u001b[1;32m    607\u001b[0m         local_worker\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    608\u001b[0m         logdir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlogdir,\n\u001b[1;32m    609\u001b[0m     )\n\u001b[1;32m    611\u001b[0m     \u001b[39m# TODO (avnishn): Remove the execution plan API by q1 2023\u001b[39;00m\n\u001b[1;32m    612\u001b[0m     \u001b[39m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    613\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_disable_execution_plan_api:\n\u001b[1;32m    614\u001b[0m         \u001b[39m# Ensure remote workers are initially in sync with the local worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:194\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, default_policy_class, config, num_workers, local_worker, logdir, _setup, policy_class, trainer_config)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[39mexcept\u001b[39;00m RayActorError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    182\u001b[0m     \u001b[39m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[39m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[39m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[39m# errors.\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[39mif\u001b[39;00m e\u001b[39m.\u001b[39mactor_init_failed:\n\u001b[1;32m    187\u001b[0m         \u001b[39m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[1;32m    188\u001b[0m         \u001b[39m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[39m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         \u001b[39m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m         \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39margs[\u001b[39m2\u001b[39m]\n\u001b[1;32m    195\u001b[0m     \u001b[39m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[39mraise\u001b[39;00m e\n",
      "\u001b[0;31mAssertionError\u001b[0m: Invalid initial state!"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=113046)\u001b[0m     self.env = env_creator(copy.deepcopy(self.env_context))\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=113046)\u001b[0m   File \"/home/spacefarers/.local/lib/python3.10/site-packages/ray/rllib/env/utils.py\", line 133, in _gym_env_creator\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=113046)\u001b[0m     env = env_descriptor(env_context)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=113046)\u001b[0m   File \"/tmp/ipykernel_88377/1908796418.py\", line 26, in reset\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=113046)\u001b[0m AssertionError: Invalid initial state!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m ValueError: The observation collected from env.reset() was not contained within your env's observation space. It is possible that there was a type mismatch, or that one of the sub-observations was out of bounds:\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m  [ 4 11  0  8]] (int32)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m  (sub-)observation space: Box(0, 3, (4, 4), int32) (int32)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=113046)\u001b[0m 2023-07-19 22:25:01,082\tERROR worker.py:861 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=113046, ip=192.168.0.121, actor_id=8bd060bcfc51582b994ed70f01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f282eadd5a0>)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m   File \"/home/spacefarers/.local/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 202, in check_gym_environments\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m \u001b[32m [repeated 12x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m AssertionError: Invalid initial state!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m AssertionError: Invalid initial state!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=113046)\u001b[0m   File \"/tmp/ipykernel_88377/1908796418.py\", line 15, in __init__\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m AssertionError: Invalid initial state!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m   File \"/home/spacefarers/.local/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 81, in check_env\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m     raise ValueError(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m AssertionError: Invalid initial state!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m AssertionError: Invalid initial state!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m AssertionError: Invalid initial state!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=112868)\u001b[0m  (sub-)obs: [[ 9  2  7  3]\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = {\n",
    "        \"framework\": \"torch\",\n",
    "        \"num_workers\": 1,\n",
    "        \"env_config\": {},\n",
    "        \"model\": {\n",
    "            \"fcnet_hiddens\": [64, 64],\n",
    "        },\n",
    "        \"num_sgd_iter\": 10,\n",
    "        \"gamma\": 0.99,\n",
    "        \"lambda\": 0.95,\n",
    "        \"clip_param\": 0.2,\n",
    "        \"lr\": 5e-4,\n",
    "        \"sgd_minibatch_size\": 128,\n",
    "        \"train_batch_size\": 2048,\n",
    "        \"rollout_fragment_length\": 256,\n",
    "        \"monitor\": True,\n",
    "    }\n",
    "stop_criteria = {\n",
    "        \"episode_reward_mean\": 1.0,  # Stop when the mean reward reaches 1.0 (solved)\n",
    "        \"time_total_s\": 600,  # Maximum training time (in seconds)\n",
    "    }\n",
    "train_agent(config, stop_criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
